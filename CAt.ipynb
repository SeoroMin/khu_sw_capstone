{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CAt.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNU7uy60cgTWvLM4U9hITlZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wwMsB3ZVQHAn"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","data = pd.read_csv('/content/drive/MyDrive/소캡디/data/preprocessed_data.csv')"],"metadata":{"id":"wOb6aP5nQgVT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pyconll"],"metadata":{"id":"oAQi0dGYR41o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# utils.py\n","\n","\"\"\"Some utils.\"\"\"\n","import pyconll\n","import numpy as np\n","\n","\n","def conll2text(paths, outpath):\n","    \"\"\"Write a conll file to a text file.\"\"\"\n","    with open(outpath, 'w') as f:\n","        for path in paths:\n","            for sent in pyconll.iter_from_file(path):\n","                txt = []\n","                for x in sent:\n","                    txt.append(x.form)\n","                if txt:\n","                    txt = \" \".join(txt).lower()\n","                    txt = \"\".join([x for x in txt if x.isprintable()])\n","                    f.write(f\"{txt}\\n\")\n","\n","\n","def normalize(x):\n","    \"\"\"Normalize a vector while controlling for zero vectors.\"\"\"\n","    x = np.copy(x)\n","    if np.ndim(x) == 1:\n","        norm = np.linalg.norm(x)\n","        if norm == 0:\n","            return x\n","        return x / np.linalg.norm(x)\n","    norm = np.linalg.norm(x, axis=-1)\n","    mask = norm > 0\n","    x[mask] /= norm[mask][:, None]\n","    return x"],"metadata":{"id":"VDOiH_F6R1qb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# fragments.py\n","\n","\"\"\"Get nouns from conllu files\"\"\"\n","import pyconll\n","import re\n","import json\n","\n","from tqdm import tqdm\n","from copy import copy\n","from collections import Counter, defaultdict\n","from itertools import chain\n","\n","\n","ARROW = re.compile(r\"(<-|->)\")\n","\n","\n","def get_fragments(path, from_pos, to_pos, max_length):\n","    \"\"\"Get all fragments from every sentence in a file.\"\"\"\n","    all_tokens = [x for x in trees_from_conll(path) if x]\n","    result = defaultdict(list)\n","    for id, tokens in all_tokens:\n","        result[id].extend(search(tokens, from_pos, to_pos, max_length))\n","    return result\n","\n","\n","def trees_from_conll(path):\n","    \"\"\"Get all trees for every sentence in a conll file.\"\"\"\n","    for x in pyconll.iter_from_file(path):\n","        yield tree(x)\n","\n","\n","def tree(s):\n","    \"\"\"Preprocess a tree to a dict.\"\"\"\n","    tokens = {t.id: {\"text\": t.form.lower(),\n","                     \"pos\": t.upos,\n","                     \"id\": t.id}\n","              for t in s}\n","    if not tokens:\n","        return s.id.split(\".\")[0], []\n","    for token in s:\n","        idx = token.id\n","        try:\n","            # ROOT has a head of None\n","            nb = token.head\n","        except (ValueError, TypeError) as e:\n","            print(e, [x.form for x in s])\n","        try:\n","            if nb != \"0\":\n","                tokens[idx][f\"<-{token.deprel}<-\"] = tokens[nb]\n","                tokens[nb][f\"->{token.deprel}->\"] = tokens[idx]\n","        except KeyError as e:\n","            print(e, tokens)\n","            return s.id.split(\".\")[0], []\n","\n","    return s.id.split(\".\")[0], list(zip(*sorted(tokens.items())))[1]\n","\n","\n","def search(tokens, from_pos, to_pos, max_length):\n","    \"\"\"\n","    Search for all patterns starting with POS tag 'f' of max_length.\n","    Parameters\n","    ----------\n","    tokens : list of dict\n","        A list of dictionaries.\n","    from_pos : string\n","        The POS tag to search from.\n","    to_pos : string\n","        The POS tag to search to.\n","    max_length : int\n","        the maximum length in dependencies to search for.\n","    Returns\n","    -------\n","    result : list\n","        A list of (word, pattern, word) triples.\n","    \"\"\"\n","    # start the search with tokens with the correct POS.\n","    result = []\n","    for token in [t for t in tokens if t[\"pos\"] == from_pos]:\n","        # return all candidates.\n","        r = []\n","        for x in list(_search(token, to_pos, 0, max_length, [], set())):\n","            pos, text = zip(*x)\n","            pos_string = \"\".join(pos)\n","            pos = ARROW.split(pos_string)\n","            c = Counter(pos)\n","            if c[from_pos] > 1 or c[to_pos] > 1:\n","                continue\n","            # print(pos, x)\n","            if pos and pos[0] == from_pos and pos[-1] == to_pos:\n","                r.append((text[0], pos_string, text[-1]))\n","        if r:\n","            result.append(sorted(r, key=lambda x: x[1])[0])\n","\n","    return result\n","\n","\n","def _search(token, to, length, max_length, path, visited, dep=None):\n","    \"\"\"Recursive function for searching trees.\"\"\"\n","    p = copy(path)\n","    if dep is None:\n","        p.append([f\"{token['pos']}\", token[\"text\"]])\n","    else:\n","        p.append([f\"{dep}{token['pos']}\", token[\"text\"]])\n","\n","    visited.add(token[\"id\"])\n","    paths = [p]\n","    if length < max_length:\n","        for k, v in [(k, v) for k, v in token.items()\n","                     if k not in {\"id\", \"pos\", \"text\"}]:\n","            if v[\"id\"] in visited:\n","                continue\n","            paths.extend(_search(v, to, length+1, max_length, p, visited, k))\n","    return [x for x in paths if x]\n","\n","\n","def create_fragments(in_files, out_path, max_length):\n","    \"\"\"Create fragments from all conllu files in a folder.\"\"\"\n","    fragments = defaultdict(list)\n","    for path in tqdm(in_files):\n","        for k, v in get_fragments(path, \"ADJ\", \"NOUN\", max_length).items():\n","            fragments[k].extend(v)\n","\n","    json.dump(fragments, open(out_path, 'w'))\n","\n","\n","def load_fragments(path_to_json,\n","                   max_path_length=5,\n","                   words=None):\n","    \"\"\"\n","    Loads fragments from a JSON file.\n","    Parameters\n","    ----------\n","    path_to_json : str\n","        The path to the json file extracted by create_fragments\n","    words : iterable\n","        An iterable of words. Only words in this set are kept.\n","    max_path_length : int, default 5\n","        The maximum path length to extract\n","    Returns\n","    -------\n","    fragments : tuple of triples\n","        A tuple consisting of (adjective, construction, noun) triples.\n","    \"\"\"\n","    fragments = json.load(open(path_to_json))\n","    _, fragments = zip(*fragments.items())\n","    fragments = list(chain(*fragments))\n","\n","    num_arrows = (max_path_length * 4) + 1\n","    fragments = [x for x in fragments\n","                 if len(ARROW.split(x[1])) <= num_arrows]\n","\n","    if words:\n","        fragments = [(x, y, z) for x, y, z in fragments if\n","                     x in words and z in words]\n","\n","    return fragments\n","\n","\n","def create_noun_counts(in_files, out_path):\n","    \"\"\"Get all noun counts.\"\"\"\n","    c = Counter()\n","    for path in in_files:\n","        c.update(nouns_from_conll(path))\n","\n","    json.dump(dict(c), open(out_path, 'w'))\n","\n","\n","def nouns_from_conll(path):\n","    \"\"\"Get all nouns, regardless of adjectival modification.\"\"\"\n","    for sent in pyconll.iter_from_file(path):\n","        for token in sent:\n","            if token.upos == \"NOUN\":\n","                yield token.form.lower()"],"metadata":{"id":"xPlNICIiSUCC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot.py\n","\n","\"\"\"Plotting of attention distributions.\"\"\"\n","from matplotlib import pyplot as plt\n","\n","\n","def plot_attention(attentions, texts):\n","    assert len(attentions) == len(texts)\n","    fig, axes = plt.subplots(len(attentions), 1, figsize=(5, 3))\n","\n","    if len(attentions) == 1:\n","        axes = [axes]\n","\n","    for idx, (att, txt) in enumerate(zip(attentions, texts)):\n","        ax = axes[idx]\n","        ax.imshow(att[None, :],\n","                  vmin=.0,\n","                  vmax=1.0,\n","                  cmap=\"Reds\",\n","                  aspect=\"auto\")\n","        ax.set_xticks(range(att.shape[0]))\n","        ax.set_xticklabels(txt, rotation=45)\n","        ax.set_yticks([])\n","\n","        for idx, x in enumerate(att):\n","            ax.text(idx-.2, 0, str(x.round(2))[1:])\n","\n","    fig.tight_layout()\n","    return fig"],"metadata":{"id":"LFPfAPgvVWro"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# simple.py\n","\n","\"\"\"Simple method.\"\"\"\n","import numpy as np\n","from collections import defaultdict\n","#from .utils import normalize\n","from sklearn.metrics.pairwise import rbf_kernel\n","from collections import Counter\n","\n","\n","def get_aspects(fragments, embeddings, n_adj_seed, n_nouns, min_count):\n","    \"\"\"Get aspects based on fragments.\"\"\"\n","    adj, _, noun = zip(*fragments)\n","    adj_cand, _ = zip(*Counter(adj).most_common(n_adj_seed))\n","\n","    cands = candidate(embeddings,\n","                      adj,\n","                      noun,\n","                      adj_cand,\n","                      n_nouns,\n","                      min_count)\n","\n","    return cands\n","\n","\n","def candidate(embeddings,\n","              adj,\n","              noun,\n","              seed_words,\n","              n_nouns,\n","              min_count):\n","    \"\"\"\n","    Generates candidate aspects based on adjective co-occurrences\n","    Parameters\n","    ----------\n","    embeddings : Reach\n","        A Reach instance containing the word embeddings.\n","    constructions : list of tuples\n","        A list of adjective noun tuples.\n","    seed_words : list of str\n","        A list of strings. All these words should be in vocab for the\n","        given embeddings model.\n","    frequency_threshold : int\n","        Any noun occurring fewer times than this threshold is discarded\n","    n_nouns : int\n","        The amount of items to return\n","    Returns\n","    -------\n","    candidates : dict\n","        A dictionary mapping strings to their scores.\n","    \"\"\"\n","    a = list(set(adj))\n","    sims = embeddings.similarity(a, seed_words).max(1)\n","    adj_scores = dict(zip(a, sims))\n","\n","    noun_scores = defaultdict(lambda: [0, 0])\n","    for adj, noun in zip(adj, noun):\n","        noun_scores[noun][0] += adj_scores[adj]\n","        noun_scores[noun][1] += 1\n","\n","    noun_scores = {k: v[0] for k, v in noun_scores.items()\n","                   if v[1] > min_count}\n","\n","    return sorted(noun_scores.items(), key=lambda x: x[1])[-n_nouns:]\n","\n","\n","def rbf_attention(vec, memory, gamma, **kwargs):\n","    \"\"\"\n","    Single-head attention using RBF kernel.\n","    Parameters\n","    ----------\n","    vec : np.array\n","        an (N, D)-shaped array, representing the tokens of an instance.\n","    memory : np.array\n","        an (M, D)-shaped array, representing the memory items\n","    gamma : float\n","        the gamma of the RBF kernel.\n","    Returns\n","    -------\n","    attention : np.array\n","        A (1, N)-shaped array, representing a single-headed attention mechanism\n","    \"\"\"\n","    z = rbf_kernel(vec, memory, gamma)\n","    s = z.sum()\n","    if s == 0:\n","        # If s happens to be 0, back off to uniform\n","        return np.ones((1, len(vec))) / len(vec)\n","    return (z.sum(1) / s)[None, :]\n","\n","\n","def softmax(x, axis=1):\n","    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n","    e_x = np.exp(x - np.max(x, axis, keepdims=True))\n","    s = e_x.sum(axis=axis, keepdims=True)\n","    return e_x / s\n","\n","\n","def attention(vec, memory, **kwargs):\n","    \"\"\"\n","    Standard multi-head attention mechanism.\n","    Parameters\n","    ----------\n","    vec : np.array\n","        an (N, D)-shaped array, representing the tokens of an instance.\n","    memory : np.array\n","        an (M, D)-shaped array, representing the memory items\n","    Returns\n","    -------\n","    attention : np.array\n","        A (M, N)-shaped array, representing the attention over all memories.\n","    \"\"\"\n","    z = memory.dot(vec.T)\n","    return softmax(z)\n","\n","\n","def mean(vec, aspect_vecs, **kwargs):\n","    \"\"\"Just a mean weighting.\"\"\"\n","    return (np.ones(len(vec)) / len(vec))[None, :]\n","\n","\n","def get_scores(instances,\n","               aspects,\n","               r,\n","               labels,\n","               remove_oov=False,\n","               attention_func=attention,\n","               **kwargs):\n","    \"\"\"Scoring function.\"\"\"\n","    assert all([x in r.items for x in labels])\n","    label_vecs = normalize(r.vectorize(labels))\n","    aspect_vecs = [x.mean(0)\n","                   for x in r.transform(aspects,\n","                                        remove_oov=False)]\n","    aspect_vecs = np.stack(aspect_vecs)\n","    if len(instances) == 1:\n","        instances = [instances]\n","\n","    t = r.transform(instances, remove_oov=remove_oov)\n","\n","    out = []\n","    for vec in t:\n","        att = attention_func(vec, aspect_vecs, **kwargs)\n","        # Att = (n_heads, n_words)\n","        z = att.dot(vec)\n","        # z = (n_heads, n_dim)\n","        x = normalize(z).dot(label_vecs.T)\n","        # x = (n_heads, n_labels)\n","        out.append(x.sum(0))\n","    return np.stack(out)"],"metadata":{"id":"--clg7cGRmDO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# preprocessing.py\n","\n","\"\"\"Creating fragments takes a long time so we treat it as a\n","pre-processing step.\"\"\"\n","import logging\n","import json\n","\n","from gensim.models import Word2Vec\n","#from cat.fragments import create_noun_counts\n","#from cat.utils import conll2text\n","from collections import Counter\n","\n","logging.basicConfig(level=logging.INFO)\n","\n","\n","if __name__ == \"__main__\":\n","\n","    paths = [\"/content/drive/MyDrive/소캡디/data/data.txt\"]\n","   \n","    create_noun_counts(paths,\n","                       \"/content/drive/MyDrive/소캡디_ABAE_Pytorch/nouns.json\")\n","    conll2text(paths, \"/content/drive/MyDrive/소캡디_ABAE_Pytorch/all_txt.txt\")\n","    corpus = [x.lower().strip().split()\n","              for x in open(\"/content/drive/MyDrive/소캡디/data/data.txt\")]\n","\n","    f = Word2Vec(corpus,\n","                 sg=0,\n","                 negative=5,\n","                 window=10,\n","                 size=200,\n","                 min_count=2,\n","                 iter=5,\n","                 workers=10)\n","\n","    f.wv.save_word2vec_format(\"/content/drive/MyDrive/소캡디_ABAE_Pytorch/my_word_vectors.vec\")\n","\n","    d = json.load(open(\"/content/drive/MyDrive/소캡디_ABAE_Pytorch/nouns.json\"))\n","    nouns = Counter()\n","    for k, v in d.items():\n","        if k.lower() in f.wv.items:\n","            nouns[k.lower()] += v\n","\n","    nouns, _ = zip(*sorted(nouns.items(),\n","                           key=lambda x: x[1],\n","                           reverse=True))\n","\n","    json.dump(nouns, open(\"/content/drive/MyDrive/소캡디_ABAE_Pytorch/aspect_words.json\", \"w\"))"],"metadata":{"id":"XZHiFZehVur4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install reach"],"metadata":{"id":"LR-ZhfJeVzav"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run.py\n","\n","import json\n","\n","#from cat.simple import get_scores, rbf_attention\n","from reach import Reach\n","from collections import defaultdict\n","\n","\n","GAMMA = .03\n","N_ASPECT_WORDS = 200\n","\n","if __name__ == \"__main__\":\n","\n","    scores = defaultdict(dict)\n","    r = Reach.load(\"embeddings/my_word_vectors.vec\",\n","                   unk_word=\"<UNK>\")\n","\n","    aspects = [[x] for x in json.load(open(\"data/aspect_words.json\"))]\n","    aspects = aspects[:N_ASPECT_WORDS]\n","\n","    instances = [\"text_1\".split(), \"text_2\".split()]\n","    label_set = {\"label1\", \"label2\", \"label3\"}\n","\n","    s = get_scores(instances,\n","                   aspects,\n","                   r,\n","                   label_set,\n","                   gamma=GAMMA,\n","                   remove_oov=False,\n","                   attention_func=rbf_attention)\n","\n","    pred = s.argmax(1)"],"metadata":{"id":"EP4Q0FhoVdtB"},"execution_count":null,"outputs":[]}]}