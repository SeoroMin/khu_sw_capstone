{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ABAE_pytorch.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"mount_file_id":"1rLM9COBDkiZSqr9RgrA0GGNBwtp1jAzQ","authorship_tag":"ABX9TyOxenETlM5AudZVESMeUujJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qR_f0BgS1LjY"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","data = pd.read_csv('/content/drive/MyDrive/소캡디/data/preprocessed_data.csv')"],"metadata":{"id":"v95pUk2-IcCB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = data.dropna()"],"metadata":{"id":"9bI_f4NyIypS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data[data['리뷰'].str.contains('채소')]"],"metadata":{"id":"SdOmopjgIj3n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_data = pd.read_csv('/content/drive/MyDrive/소캡디/data/skin_top10_review.csv')\n","raw_data[raw_data['리뷰'].str.contains('채소')]"],"metadata":{"id":"6Z9gjyPFJBzf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_data.replace('올리브영', '', inplace=True)\n","raw_data.replace('채소', '', inplace=True)\n","raw_data.replace('증인', '', inplace=True)\n","raw_data.replace('넘버', '', inplace=True)"],"metadata":{"id":"6Ls7vd3AqBxH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_data"],"metadata":{"id":"gwJAdkBvra2Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_data[raw_data['리뷰'].str.contains('채소')]"],"metadata":{"id":"Tqc_99hYrfzf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model.py\n","\n","# -*- coding: utf-8 -*-\n","import numpy as np\n","import torch\n","from torch.nn import init\n","from torch.nn.parameter import Parameter\n","\n","\n","class SelfAttention(torch.nn.Module):\n","    def __init__(self, wv_dim: int, maxlen: int):\n","        super(SelfAttention, self).__init__()\n","        self.wv_dim = wv_dim\n","\n","        # max sentence length -- batch 2nd dim size\n","        self.maxlen = maxlen\n","        self.M = Parameter(torch.empty(size=(wv_dim, wv_dim)))\n","        init.kaiming_uniform_(self.M.data)\n","\n","        # softmax for attending to wod vectors\n","        self.attention_softmax = torch.nn.Softmax(dim=-1)\n","\n","    def forward(self, input_embeddings):\n","        # (b, wv, 1)\n","        mean_embedding = torch.mean(input_embeddings, (1,)).unsqueeze(2)\n","\n","        # (wv, wv) x (b, wv, 1) -> (b, wv, 1)\n","        product_1 = torch.matmul(self.M, mean_embedding)\n","\n","        # (b, maxlen, wv) x (b, wv, 1) -> (b, maxlen, 1)\n","        product_2 = torch.matmul(input_embeddings, product_1).squeeze(2)\n","\n","        results = self.attention_softmax(product_2)\n","\n","        return results\n","\n","    def extra_repr(self):\n","        return 'wv_dim={}, maxlen={}'.format(self.wv_dim, self.maxlen)\n","\n","\n","class ABAE(torch.nn.Module):\n","    \"\"\"\n","        The model described in the paper ``An Unsupervised Neural Attention Model for Aspect Extraction''\n","        by He, Ruidan and  Lee, Wee Sun  and  Ng, Hwee Tou  and  Dahlmeier, Daniel, ACL2017\n","        https://aclweb.org/anthology/papers/P/P17/P17-1036/\n","    \"\"\"\n","\n","    def __init__(self, wv_dim: int = 200, asp_count: int = 30,\n","                 ortho_reg: float = 0.1, maxlen: int = 201, init_aspects_matrix=None):\n","        \"\"\"\n","        Initializing the model\n","        :param wv_dim: word vector size\n","        :param asp_count: number of aspects\n","        :param ortho_reg: coefficient for tuning the ortho-regularizer's influence\n","        :param maxlen: sentence max length taken into account\n","        :param init_aspects_matrix: None or init. matrix for aspects\n","        \"\"\"\n","        super(ABAE, self).__init__()\n","        self.wv_dim = wv_dim\n","        self.asp_count = asp_count\n","        self.ortho = ortho_reg\n","        self.maxlen = maxlen\n","\n","        self.attention = SelfAttention(wv_dim, maxlen)\n","        self.linear_transform = torch.nn.Linear(self.wv_dim, self.asp_count)\n","        self.softmax_aspects = torch.nn.Softmax(dim=-1)\n","        self.aspects_embeddings = Parameter(torch.empty(size=(wv_dim, asp_count)))\n","\n","        if init_aspects_matrix is None:\n","            torch.nn.init.xavier_uniform(self.aspects_embeddings)\n","        else:\n","            self.aspects_embeddings.data = torch.from_numpy(init_aspects_matrix.T)\n","\n","    def get_aspects_importances(self, text_embeddings):\n","        \"\"\"\n","            Takes embeddings of a sentence as input, returns attention weights\n","        \"\"\"\n","\n","        # compute attention scores, looking at text embeddings average\n","        attention_weights = self.attention(text_embeddings)\n","\n","        # multiplying text embeddings by attention scores -- and summing\n","        # (matmul: we sum every word embedding's coordinate with attention weights)\n","        weighted_text_emb = torch.matmul(attention_weights.unsqueeze(1),  # (batch, 1, sentence)\n","                                         text_embeddings  # (batch, sentence, wv_dim)\n","                                         ).squeeze()\n","\n","        # encoding with a simple feed-forward layer (wv_dim) -> (aspects_count)\n","        raw_importances = self.linear_transform(weighted_text_emb)\n","\n","        # computing 'aspects distribution in a sentence'\n","        aspects_importances = self.softmax_aspects(raw_importances)\n","\n","        return attention_weights, aspects_importances, weighted_text_emb\n","\n","    def forward(self, text_embeddings, negative_samples_texts):\n","\n","        # negative samples are averaged\n","        averaged_negative_samples = torch.mean(negative_samples_texts, dim=2)\n","\n","        # encoding: words embeddings -> sentence embedding, aspects importances\n","        _, aspects_importances, weighted_text_emb = self.get_aspects_importances(text_embeddings)\n","\n","        # decoding: aspects embeddings matrix, aspects_importances -> recovered sentence embedding\n","        recovered_emb = torch.matmul(self.aspects_embeddings, aspects_importances.unsqueeze(2)).squeeze()\n","\n","        # loss\n","        reconstruction_triplet_loss = ABAE._reconstruction_loss(weighted_text_emb,\n","                                                                recovered_emb,\n","                                                                averaged_negative_samples)\n","        max_margin = torch \\\n","            .max(reconstruction_triplet_loss, torch.zeros_like(reconstruction_triplet_loss)) \\\n","            .unsqueeze(dim=-1)\n","\n","        return self.ortho * self._ortho_regularizer() + max_margin\n","\n","    @staticmethod\n","    def _reconstruction_loss(text_emb, recovered_emb, averaged_negative_emb):\n","\n","        positive_dot_products = torch.matmul(text_emb.unsqueeze(1), recovered_emb.unsqueeze(2)).squeeze()\n","        negative_dot_products = torch.matmul(averaged_negative_emb, recovered_emb.unsqueeze(2)).squeeze()\n","        reconstruction_triplet_loss = torch.sum(1 - positive_dot_products.unsqueeze(1) + negative_dot_products, dim=1)\n","\n","        return reconstruction_triplet_loss\n","\n","    def _ortho_regularizer(self):\n","        return torch.norm(\n","            torch.matmul(self.aspects_embeddings.t(), self.aspects_embeddings) \\\n","            - torch.eye(self.asp_count))\n","\n","    def get_aspect_words(self, w2v_model, logger, topn=15):\n","        words = []\n","\n","        # getting aspects embeddings\n","        aspects = self.aspects_embeddings.detach().numpy()\n","\n","        # getting scalar products of word embeddings and aspect embeddings;\n","        # to obtain the ``probabilities'', one should also apply softmax\n","        # words_scores = w2v_model.wv.syn0.dot(aspects)\n","        words_scores = w2v_model.wv.vectors.dot(aspects)\n","\n","        for row in range(aspects.shape[1]):\n","            argmax_scalar_products = np.argsort(- words_scores[:, row])[:topn]\n","            # print([w for w, dist in w2v_model.wv.similar_by_vector(aspects.T[row])[:topn]])\n","            words.append([w2v_model.wv.index_to_key[i] for i in argmax_scalar_products])\n","\n","        return words"],"metadata":{"id":"UFnNA9AE1bS3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import codecs\n","import sys\n","\n","import gensim\n","from tqdm import tqdm\n","\n","model = gensim.models.Word2Vec.load('/content/drive/MyDrive/소캡디/data/w2v_embedding.model')"],"metadata":{"id":"hGK-Y08K1yx4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install gensim==4.0.0"],"metadata":{"id":"zXJGT81CGFSq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# reader.py\n","\n","# -*- coding: utf-8 -*-\n","import gensim\n","import numpy as np\n","from sklearn.cluster import MiniBatchKMeans\n","\n","\n","def read_data_batches(path: str, batch_size: int=50, minlength: int=5):\n","    \"\"\"\n","        Reading batched texts of given min. length\n","    :param path: path to the text file ``one line -- one normalized sentence''\n","    :return: batches iterator\n","    \"\"\"\n","    batch = []\n","\n","    for line in open(path, encoding=\"utf-8\"):\n","        line = line.strip().split()\n","\n","        # lines with less than `minlength` words are omitted\n","        if len(line) >= minlength:\n","            batch.append(line)\n","            if len(batch) >= batch_size:\n","                yield batch\n","                batch = []\n","\n","    if len(batch) > 0:\n","        yield batch\n","\n","\n","def text2vectors(text: list, w2v_model, maxlen: int, vocabulary):\n","    \"\"\"\n","        Token sequence -- to a list of word vectors;\n","        if token not in vocabulary, it is skipped; the rest of\n","        the slots up to `maxlen` are replaced with zeroes\n","    :param text: list of tokens\n","    :param w2v_model: gensim w2v model\n","    :param maxlen: max. length of the sentence; the rest is just cut away\n","    :return:\n","    \"\"\"\n","\n","    acc_vecs = []\n","\n","    for word in text:\n","        if word in w2v_model.wv and (vocabulary is None or word in vocabulary):\n","            acc_vecs.append(w2v_model.wv[word])\n","\n","    # padding for consistent length with ZERO vectors\n","    if len(acc_vecs) < maxlen:\n","        acc_vecs.extend([np.zeros(w2v_model.vector_size)] * (maxlen - len(acc_vecs)))\n","\n","    return acc_vecs\n","\n","\n","def get_w2v(path):\n","    \"\"\"\n","        Reading word2vec model given the path\n","    \"\"\"\n","    return gensim.models.Word2Vec.load(path)\n","\n","\n","def read_data_tensors(path, word_vectors_path=None,\n","                      batch_size=50, vocabulary=None,\n","                      maxlen=100, pad_value=0, min_sent_length=5):\n","    \"\"\"\n","        Data for training the NN -- from text file to word vectors sequences batches\n","    :param path:\n","    :param word_vectors_path:\n","    :param batch_size:\n","    :param vocabulary:\n","    :param maxlen:\n","    :param pad_value:\n","    :param minsentlength:\n","    :return:\n","    \"\"\"\n","    w2v_model = get_w2v(word_vectors_path)\n","\n","    for batch in read_data_batches(path, batch_size, min_sent_length):\n","        batch_vecs = []\n","        batch_texts = []\n","\n","        for text in batch:\n","            vectors_as_list = text2vectors(text, w2v_model, maxlen, vocabulary)\n","            batch_vecs.append(np.asarray(vectors_as_list[:maxlen], dtype=np.float32))\n","            batch_texts.append(text)\n","\n","        yield np.stack(batch_vecs, axis=0), batch_texts\n","\n","\n","def get_centroids(w2v_model, aspects_count):\n","    \"\"\"\n","        Clustering all word vectors with K-means and returning L2-normalizes\n","        cluster centroids; used for ABAE aspects matrix initialization\n","    \"\"\"\n","\n","    km = MiniBatchKMeans(n_clusters=aspects_count, verbose=0, n_init=100)\n","    m = []\n","\n","    for k in w2v_model.wv.key_to_index:\n","        m.append(w2v_model.wv[k])\n","\n","    m = np.matrix(m)\n","    km.fit(m)\n","    clusters = km.cluster_centers_\n","\n","    # L2 normalization\n","    norm_aspect_matrix = clusters / np.linalg.norm(clusters, axis=-1, keepdims=True)\n","\n","    return norm_aspect_matrix\n","\n","\n","if __name__ == \"__main__\":\n","\n","    for b in read_data_tensors(\"/content/drive/MyDrive/소캡디/data/data.txt\", '/content/drive/MyDrive/소캡디/data/w2v_embedding.model', batch_size=3):\n","        print(b[0].shape, b[1][:2])"],"metadata":{"id":"Ksd7DCtW2Qon"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install hydra"],"metadata":{"id":"6aIMEJus45du"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install hydra-core --upgrade"],"metadata":{"id":"tWL453jT5ly9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","import logging\n","\n","import pathlib\n","#import hydra\n","import numpy as np\n","import torch\n","import os\n","import argparse\n","\n","#from model import ABAE\n","#from reader import get_centroids, get_w2v, read_data_tensors\n","\n","\n","logger = logging.getLogger(__name__)\n","\n","#parser = argparse.ArgumentParser()\n","#args = parser.parse_args(args=[])\n","#@hydra.main(\"configs\", \"config\")\n","def main():\n","    w2v_model = get_w2v(os.path.join('/content/drive/MyDrive/소캡디/data/w2v_embedding.model'))\n","    wv_dim = w2v_model.vector_size\n","    y = torch.zeros((8, 1))\n","\n","    model = ABAE(wv_dim=wv_dim,\n","                 asp_count = 5,\n","                 init_aspects_matrix=get_centroids(w2v_model, aspects_count = 5))\n","    logger.debug(str(model))\n","    \n","\n","    criterion = torch.nn.MSELoss(reduction=\"sum\")\n","    optimizer = torch.optim.Adam(model.parameters())\n","    \n","\n","    for t in range(1):\n","\n","        logger.debug(\"Epoch %d/%d\" % (t + 1, 1))\n","\n","        data_iterator = read_data_tensors('/content/drive/MyDrive/소캡디/data/data.txt',\n","                                          '/content/drive/MyDrive/소캡디/data/w2v_embedding.model',\n","                                          batch_size=8, maxlen=201)\n","\n","        for item_number, (x, texts) in enumerate(data_iterator):\n","            if x.shape[0] < 8:  # pad with 0 if smaller than batch size\n","                x = np.pad(x, ((0, 8 - x.shape[0]), (0, 0), (0, 0)))\n","\n","            x = torch.from_numpy(x)\n","\n","            # extracting bad samples from the very same batch; not sure if this is OK, so todo\n","            negative_samples = torch.stack(\n","                tuple([x[torch.randperm(x.shape[0])[:5]]\n","                       for _ in range(8)]))\n","\n","            # prediction\n","            y_pred = model(x, negative_samples)\n","            #print(y_pred)\n","\n","            # error computation\n","            loss = criterion(y_pred, y)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            if item_number % 100 == 0:\n","\n","                print(\"%d batches, and LR: %.5f\" % (item_number, optimizer.param_groups[0]['lr']))\n","\n","                for i, aspect in enumerate(model.get_aspect_words(w2v_model, logger)):\n","                    print((\"[%d] %s\" % (i + 1, \" \".join([a for a in aspect]))))\n","                    #print(aspect)\n","\n","                print(\"Loss: %.4f\" % loss.item())\n","\n","                try:\n","                    torch.save(model, f\"abae_%.2f_%06d.bin\" % (loss.item(), item_number))\n","                    print(1)\n","                except Exception as e:\n","                    print(\"Model saving failed.\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"Vxk0gwV14wE5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ㄴS'''\n","    if cfg.optimizer.name == \"adam\":\n","        optimizer = torch.optim.Adam(model.parameters())\n","    elif cfg.optimizer.name == \"sgd\":\n","        optimizer = torch.optim.SGD(model.parameters(), lr=cfg.optimizer.learning_rate)\n","    elif cfg.optimizer.name == \"adagrad\":\n","        optimizer = torch.optim.Adagrad(model.parameters())\n","    elif cfg.optimizer.name == \"asgd\":\n","        optimizer = torch.optim.ASGD(model.parameters(), lr=cfg.optimizer.learning_rate)\n","    else:\n","        raise Exception(\"Optimizer '%s' is not supported\" % cfg.optimizer.name)\n","'''\n"],"metadata":{"id":"YJ8etX9V4FB3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"p8_atVOEbpZC"},"execution_count":null,"outputs":[]}]}